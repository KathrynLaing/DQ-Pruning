# Experiment
As detailed in our paper, ARXIV LINK, we conducted an experiment to compare the performance of several dominance testing functions for CP-Nets.
Each dominance testing function answers dominance queries by building the associated search tree.
To improve efficiency, a pruning method is used to prune certain branches of this tree as it is constructed.
In order to fully specify how a dominance testing function works, one must also specify the method of leaf prioritisation used (when building up the tree).
Which pruning method and prioritisation technique combinations we tested are given below.
The performance of a dominance testing function on a given query is measured both by outcomes traversed and time elapsed.
For full details of this terminology, we refer you to section 6 of the paper.

Given *n* (number of variables in the CP-net), each dominance testing function was tested on the same set of 1000 dominance queries.
These queries were randomly generated by generating 100 CP-nets and then for each, 10 queries were randomly generated.

This was done twice. First, for the case of binary CP-nets, for *n*=3-10.
Second, for the case of multivalued CP-nets, for *n*=3-8. By multivalued CP-nets, we mean that the variables could have domain size between 2-5.

In this repository, we provide the CP-net generator code we used. We also provide the code for each of the dominance testing functions.
These are all provided as R scripts and allow you to repeat these experiments with different values of *n*.
You may also vary the values of the maximum possible size of variable domain and the maximum number of parents a variable can have (we left this as *(n-1)* in our experiments).
We also provide the raw results of our experiments, as well as the generated graphs of the average performance of the different functions.

# CP-Net Generator
The CP-net generator we used for these experiments takes some inspiration from the CP-net generator described in (Allen et al., 2016). In particular, the usage of the dagcode representation of DAGs, and the test for conditional preference table (CPT) degeneracy. However, our generator does not guarantee a specific distribution over the generated CP-nets. This is because the probability vectors required by Allen et al. (2016), to make their generation uniform, required more precision than was possible for our computational resources. Our generator also differs from theirs in that it allows variables to have different domain sizes (whereas Allen et al. (2016) requires all variables to have the same domain size).

The CP-net generator function, `rand.cpn` is given in the R script `CPNGenerator.R`, along with all of its dependent functions. Note that these functions require the R libraries `primes` and `Rmpfr`. In this script, we also illustrate through an example what the output CP-nets look like as R objects. We also give an example of generating an associated outcome. These are the required inputs for the following dominance testing functions.

The CP-net generator function works as follows:
1. Generate a valid dagcode
2. Convert this dagcode to the adjacency matrix, *A*, of the associated DAG
3. `for` each variable: Generate a random CPT
4. `if` any of these CPTs are degenerate `then` go back to step 3
5. Return the CP-net as the pair (*A*, *CPT*) where *CPT* is a list of the generated CPTs

For the theory of dagcodes, obtaining DAGs from dagcodes, and their equivalence, we refer you to Allen et al. (2016). A CPT, say the CPT of variable *X* (CPT(*X*)), is degenerate, or invalid, if it implies that one of the parents of *X* is not valid. A parent, *Y*, of *X* is valid if you can change the user's preference over *X* by changing the value taken by *Y only*. Thus, a CPT of *X* is valid (non-degenerate) if, for every parent *Y*, there are two rows where the parental assignment differs only on the value taken by *Y* and the preference over *X* is different in these two rows.

`rand.cpn(n,p,d)` outputs a random CP-net with *n* variables, each of which has domain size in 2-*d*, and each of which has no more than *p* parents.

# Dominance Testing Functions
## Pruning Methods
We tested three different pruning methods:
* Rank Pruning - ARXIV LINK
* Penalty Pruning - (Li et al., 2011)
* Suffix Fixing - (Boutilier et al., 2004)

We tested each of these methods used individually, all possible pairwise combinations, and all three methods used together.
This gives us 7 pruning schema options.
Again, we refer you to section 6 of ARXIV LINK for a detailed description of how these three pruning methods work as well as how they are combined.

## Leaf Prioritisation methods
There have been several methods of leaf prioritisation suggested in the existing literature.
However, there has been no experimental analysis of how effective the different methods are.
We decided, for the purposes of keeping our dominance testing functions efficient, to only implement leaf prioritisation methods that do not require additional calculations.
Thus, we consider the following prioritisation heuristics:
* Minimal Depth - Simply selects a leaf at minimal depth in the current search tree.
* Penalty Prioritisation (Li et al., 2011) - Selects a leaf which has minimal *f* (evaluation function for penalty pruning) value.
* Rank Prioritisation - Selects a leaf with maximal rank, *r*, value.
* Rank + Difference Prioritisation - Selects a leaf, *o*, with minimal *r(o) + L_D(o, o_1)* value (for the query '*o_1>o_2*?').

The latter two are our suggested heuristics, based upon our rank values introduced in ARXIV LINK.
Search directions with higher rank (or rank + diff.) values are more likely to either be successful in reaching o_1 or to terminate quickly.
Thus, it makes sense to prioritise these directions.

## Combinations
As we mentioned above, only prioritisation methods that do not require additional calculations were tested.
Below we show, for each pruning schema, which leaf prioritisation methods were tested.
Note that, as minimal depth is not an intelligent heuristic, it was tested only for the pruning method that did not permit the others (suffix fixing alone).

(R- rank pruning, P- penalty pruning, S- suffix fixing)
* *Pruning Method - Tested Prioritisation Hueristics*
* R - Rank Prioritisation, Rank + Diff. Prioritisation
* P - Penalty Prioritisation
* S - Min. Depth
* R & P - Rank Prioritisation, Rank + Diff. Prioritisation, Penalty Prioritisation
* R & S - Rank Prioritisation, Rank + Diff. Prioritisation
* P & S - Penalty Prioritisation
* R & P & S - Rank Prioritisation, Rank + Diff. Prioritisation, Penalty Prioritisation

This makes the total number of tested functions 13.

## R Functions
The R functions to answer dominance queries using these different functions are given in the R Script `DominanceTestingFunctions.R`.
Suppose we wish to answer the dominance query 'o1>o2 ?' (o1, o2 outcomes) for the CP-net N.
Let `A` be the adjacency matrix of the structure of N and let `CPT` be a list of the conditional preference tables (CPTs) of N.
To answer this dominance query using the different pruning and prioritisation combinations above, use the following R functions, found in `DominanceTestingFunctions.R`:

* **Rank Pruning, Rank Prioritisation**\
`DQ.Rank(o1,o2,A,CPT,priority="rank",suffix=FALSE,dig)`
* **Rank Pruning, Rank + Difference Prioritisation**\
`DQ.Rank(o1,o2,A,CPT,priority="rank.diff",suffix=FALSE,dig)`
* **Penalty Pruning, Penalty Prioritisation**\
`DQ.Penalty(o1,o2,A,CPT,suffix=FALSE,dig)`
* **Suffix Fixing, Minimal Depth Prioritisation**\
`DQ.SF(o1,o2,A,CPT,dig)`
* **Rank Pruning + Penalty Pruning, Rank Prioritisation**\
`DQ.PR(o1,o2,A,CPT,priority="rank",suffix=FALSE,dig)`
* **Rank Pruning + Penalty Pruning, Rank + Difference Prioritisation**\
`DQ.Rank(o1,o2,A,CPT,priority="rank.diff",suffix=FALSE,dig)`
* **Rank Pruning + Penalty Pruning, Penalty Prioritisation**\
`DQ.Rank(o1,o2,A,CPT,priority="penalty",suffix=FALSE,dig)`
* **Rank Pruning + Suffix Fixing, Rank Prioritisation**\
`DQ.Rank(o1,o2,A,CPT,priority="rank",suffix=TRUE,dig)`
* **Rank Pruning + Suffix Fixing, Rank + Difference Prioritisation**\
`DQ.Rank(o1,o2,A,CPT,priority="rank.diff",suffix=TRUE,dig)`
* **Penalty Pruning + Suffix Fixing, Penalty Prioritisation**\
`DQ.Penalty(o1,o2,A,CPT,suffix=TRUE,dig)`
* **Rank Pruning + Penalty Pruning + Suffix Fixing, Rank Prioritisation**\
`DQ.PR(o1,o2,A,CPT,priority="rank",suffix=TRUE,dig)`
* **Rank Pruning + Penalty Pruning + Suffix Fixing, Rank + Difference Prioritisation**\
`DQ.Rank(o1,o2,A,CPT,priority="rank.diff",suffix=TRUE,dig)`
* **Rank Pruning + Penalty Pruning + Suffix Fixing, Penalty Prioritisation**\
`DQ.Rank(o1,o2,A,CPT,priority="penalty",suffix=TRUE,dig)`

Note that `dig` is a specified level of precision. How this is calculated, for the CP-net of interest, is given in `DominanceTestingFunctions.R`. Further, each of these functions relies on a set of minor functions (`DP`,`ancestor`,`n.val`,`Rank`,`Penalty`,and `Pen.Rank`) that are given at the start of `DominanceTestingFunctions.R` and must be loaded before the dominance testing functions may be used. These functions also require two R libraries, `primes` and `Rmpfr`.

These dominance testing functions output the outcome of the dominance query, the number of outcomes traversed, and the time taken to answer the query.

**REFERENCES**\
Allen, T.E., Goldsmith, G., Justice, H.E., Mattei, N., and Raines, K. (2016). Generating CP-nets Uniformly at Random. *Proc. of 30th Annual Converence of the Association for the Advancement of Artificial Intelligence*, pages 872-878, Arizona, USA.

Boutilier, C., Brafman, R. I., Domshlak, C., Hoos, H. H., and Poole, D. (2004). CP-nets: A tool for representing and reasoning with conditional *Ceteris Paribus* preference statements. *Journal of Artifcial Intelligence Research*, 21:135-191.

Li, M., Vo, Q. B., and Kowalczyk, R. (2011). Efficient heuristic approach to dominance testing in CP-nets. In Tumer, Yolum, Sonenberg, and Stone, editors, *Proc. of 10th International Conference on Autonomous Agents and Multiagent Systems*, pages 353-360, Taipei, Taiwan.
